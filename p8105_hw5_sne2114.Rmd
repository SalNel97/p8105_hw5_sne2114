---
title: "Homework 5"
author: "Salah El-Sadek (sne2114)"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
library(tidyverse)
library(rvest)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

```{r}
homicide_data = read_csv("./homicide-data.csv")
```


This is data on over 50,000 homicides across 50 large cities in the U.S. collected by the Washington Post.
There are `r nrow(homicide_data)` rows and `r ncol(homicide_data)` columns overall.\
Variables in the data include: ***`r names(homicide_data)`***

Next, we create a city_state variable using data on city and state.
```{r}
homicide_df = 
  homicide_data %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")

homicide_df
```

Here we obtain the total number of homicides and total number of unsolved homicides (no arrest made).

```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )

aggregate_df
```

Using prop.test to estimate proportion of homicides that went unsolved in Baltimore, MD.

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

Iterating the prop.test for all cities.

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)

results_df
```

Plotting the proportion of unsolved homicides with error bars for each city.

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


## Problem 2

Loading the data and merging each spreadsheet into a list while preserving the file names containing ID and study group status.

```{r}
study_df = 
  tibble(
    path = list.files("data")) %>% 
  mutate(
    path = str_c("data/", path),
    data = map(path, read_csv)
    ) %>%
  unnest(data) %>%
  mutate(path = str_replace(path, "data/", " "),
         path = str_replace(path, ".csv", " ")) %>%
  separate(path, into = c("group", "subjectID"), sep = "_") 
```

Using pivot_longer so that we can have week number and observations as separate columns

```{r}
study_df =
  study_df %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "Week", 
    names_prefix = "week_", 
    values_to = "Observations"
  ) %>%
  mutate(
    group = str_replace(group, "con", "Control"),
    group = str_replace(group, "exp", "Experimental")
  ) %>%
  relocate(subjectID) %>%
  mutate(
    week = as.numeric(Week)
  )

study_df
```

Spaghetti plot for observations on each subject over time.

```{r}
study_df %>% 
  ggplot(aes(x = Week, y = Observations, color = group)) +
  geom_path(aes(group = subjectID)) +
  geom_point() +
  labs(
      title = "Observations for each Subject Over Time",
      x = "Week",
      y = "Observations",
      color = "Group: ")
```

The experimental group has higher values for observations overall compared to the control group.
Also, the values for observations seems to increase over time for the experimental group compared to the control.

## Problem 3

Generate a normal distribution with n = 30, mean of 0, and standard deviation of 5.
We will then run a t-test with mu = 0 and rerun to generate 5000 data sets (along with their estimates and p-value).

```{r}
sim_mean_p = function(n = 30, mu, sigma = 5) {
  
  sim_data = 
    tibble(
    x = rnorm(n = n, mean = mu, sd = sigma),
  ) %>%
    t.test() %>%
    broom::tidy()
  
  sim_data %>% 
    select(estimate, p.value)
}

sim_results =
  rerun(5000, sim_mean_p(mu = 0)) %>% 
  bind_rows()

sim_results %>% 
  select(estimate, p.value)
```

We now repeat for mu = 1,2,3,4,5,6.

```{r}
multi_means = 
  tibble(multi_mu = c(0, 1, 2, 3, 4, 5, 6)) %>%
  mutate(
    output = map(.x = multi_mu, ~rerun(5000, sim_mean_p(mu = .x))),
    estimate = map(output, bind_rows)) %>% 
  select(-output) %>% 
  unnest()

multi_means
```

Plotting the proportion of times a false null hypothesis was rejected (power) versus effect size.
We can see that effect size is positively correlated with power. 
We begin reaching the max power of 1 starting with an effect size of 4.

```{r}
multi_means %>%
  filter(p.value < 0.05) %>%
  group_by(multi_mu) %>%
  count() %>%
  mutate(power = n/5000) %>%
  ggplot(aes(x = multi_mu, y = power, color = multi_mu)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Association between Power and Effect Size",
    x = "True Value of Mu",
    y = "Power",
    color = "True Value of Mu: "
  )
```

plot

```{r}
all_samples_p = 
  multi_means %>%
   mutate(
    multi_mu = str_c(" ", multi_mu)) %>%
  ggplot(aes(x = multi_mu, y = estimate, fill = multi_mu)) +
  geom_violin(alpha = .5) +
  labs(
    title = "All Samples",
    x = "True Value of Mean",
    y = "Average Estimate of the Mean") +
  theme(legend.position = "none") + 
  stat_summary(fun = "mean")

all_samples_p
```

